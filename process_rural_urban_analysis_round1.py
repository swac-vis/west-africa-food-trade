#!/usr/bin/env python3
"""
Enhanced food flows processing with Rural-Urban analysis
Adds classification of flows by rural/urban patterns
Now with OSRM real route paths
New hierarchical data structure by year

USAGE:
  Basic (no OSRM):          python process_rural_urban_analysis.py
  With OSRM (all routes):   python process_rural_urban_analysis.py --osrm
  With OSRM (top 100):      python process_rural_urban_analysis.py --osrm --top100

CRASH RECOVERY:
  - OSRM cache is saved every 50 routes to 'osrm_route_cache.json'
  - Progress is tracked in 'osrm_progress.json'
  - Intermediate results saved to '*_temp.json'
  - If interrupted (Ctrl+C or crash), simply run the same command again
  - Script will ask if you want to resume from where it stopped
  
FEATURES:
  - âœ“ Automatic resume from last checkpoint
  - âœ“ Progress tracking every 50 routes
  - âœ“ Safe Ctrl+C interrupt (saves before exit)
  - âœ“ Crash recovery with intermediate files
  - âœ“ OSRM route caching (no duplicate API calls)
"""

import pandas as pd
import json
import requests
import time
import os
import sys
from pathlib import Path
from collections import defaultdict
from datetime import datetime, timedelta

# OSRMè·¯å¾„ç¼“å­˜
ROUTE_CACHE = {}
CACHE_FILE = 'osrm_route_cache_round1.json'  # round1 ä¸“ç”¨ç¼“å­˜
PROGRESS_FILE = 'osrm_progress_round1.json'  # round1 ä¸“ç”¨è¿›åº¦

def load_route_cache():
    """åŠ è½½OSRMè·¯å¾„ç¼“å­˜"""
    global ROUTE_CACHE
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, 'r') as f:
            ROUTE_CACHE = json.load(f)
        print(f"âœ“ Loaded {len(ROUTE_CACHE)} cached routes")
    return ROUTE_CACHE

def save_route_cache():
    """ä¿å­˜OSRMè·¯å¾„ç¼“å­˜"""
    with open(CACHE_FILE, 'w') as f:
        json.dump(ROUTE_CACHE, f, indent=2)
    print(f"ğŸ’¾ Saved {len(ROUTE_CACHE)} routes to cache")

def load_progress():
    """åŠ è½½å¤„ç†è¿›åº¦"""
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, 'r') as f:
            progress = json.load(f)
        print(f"âœ“ Found previous progress: {progress['processed']}/{progress['total']} routes processed")
        return progress
    return None

def save_progress(processed, total, last_route_id):
    """ä¿å­˜å¤„ç†è¿›åº¦"""
    progress = {
        'processed': processed,
        'total': total,
        'last_route_id': last_route_id,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    with open(PROGRESS_FILE, 'w') as f:
        json.dump(progress, f, indent=2)

def save_intermediate_results(data, filename='food_flows_by_year_temp.json'):
    """ä¿å­˜ä¸­é—´ç»“æœï¼ˆé˜²æ­¢æ•°æ®ä¸¢å¤±ï¼‰"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ Saved intermediate results to {filename}")

def simplify_path(points, epsilon=0.001):
    """
    ä½¿ç”¨ Douglas-Peucker ç®—æ³•ç®€åŒ–è·¯å¾„
    epsilon: ç®€åŒ–é˜ˆå€¼ï¼ˆåº¦æ•°ï¼‰ï¼Œ0.001 çº¦ç­‰äº 111 ç±³
    """
    if len(points) < 3:
        return points
    
    def perpendicular_distance(point, line_start, line_end):
        """è®¡ç®—ç‚¹åˆ°çº¿æ®µçš„å‚ç›´è·ç¦»"""
        x0, y0 = point
        x1, y1 = line_start
        x2, y2 = line_end
        
        # çº¿æ®µé•¿åº¦
        dx = x2 - x1
        dy = y2 - y1
        
        if dx == 0 and dy == 0:
            return ((x0 - x1)**2 + (y0 - y1)**2)**0.5
        
        # ç‚¹åˆ°çº¿çš„è·ç¦»
        t = max(0, min(1, ((x0 - x1) * dx + (y0 - y1) * dy) / (dx * dx + dy * dy)))
        proj_x = x1 + t * dx
        proj_y = y1 + t * dy
        
        return ((x0 - proj_x)**2 + (y0 - proj_y)**2)**0.5
    
    def rdp(points, epsilon):
        """é€’å½’ Douglas-Peucker ç®—æ³•"""
        if len(points) < 3:
            return points
        
        # æ‰¾åˆ°è·ç¦»èµ·ç‚¹-ç»ˆç‚¹è¿çº¿æœ€è¿œçš„ç‚¹
        max_dist = 0
        max_index = 0
        
        for i in range(1, len(points) - 1):
            dist = perpendicular_distance(points[i], points[0], points[-1])
            if dist > max_dist:
                max_dist = dist
                max_index = i
        
        # å¦‚æœæœ€å¤§è·ç¦»å¤§äºé˜ˆå€¼ï¼Œé€’å½’ç®€åŒ–
        if max_dist > epsilon:
            left = rdp(points[:max_index + 1], epsilon)
            right = rdp(points[max_index:], epsilon)
            return left[:-1] + right
        else:
            return [points[0], points[-1]]
    
    simplified = rdp(points, epsilon)
    return simplified

def print_progress_bar(current, total, success, fail, start_time, prefix='Progress'):
    """
    æ‰“å°è¿›åº¦æ¡å’Œç»Ÿè®¡ä¿¡æ¯
    """
    # è®¡ç®—ç™¾åˆ†æ¯”
    percent = 100 * (current / float(total))
    filled_length = int(50 * current // total)
    bar = 'â–ˆ' * filled_length + 'â–‘' * (50 - filled_length)
    
    # è®¡ç®—é€Ÿåº¦å’ŒETA
    elapsed = time.time() - start_time
    if current > 0:
        rate = current / elapsed
        eta_seconds = (total - current) / rate if rate > 0 else 0
        eta_str = str(timedelta(seconds=int(eta_seconds)))
        speed_str = f"{rate:.1f} routes/s"
    else:
        eta_str = "calculating..."
        speed_str = "0 routes/s"
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = f"âœ“{success} âœ—{fail}"
    
    # æ‰“å°è¿›åº¦æ¡ï¼ˆä½¿ç”¨ \r è¦†ç›–åŒä¸€è¡Œï¼‰
    sys.stdout.write(f'\r   {prefix}: |{bar}| {percent:.1f}% ({current}/{total}) {stats} | Speed: {speed_str} | ETA: {eta_str}')
    sys.stdout.flush()

def get_osrm_route(source_coords, via_coords, dest_coords, route_id):
    """
    è·å–ç»è¿‡ä¸‰ç‚¹çš„OSRMè·¯å¾„
    
    Args:
        source_coords: [lon, lat] èµ·ç‚¹
        via_coords: [lon, lat] ç»è¿‡ç‚¹
        dest_coords: [lon, lat] ç»ˆç‚¹
        route_id: è·¯çº¿IDç”¨äºç¼“å­˜
    
    Returns:
        {
            'path': [[lon, lat], ...],  # è·¯å¾„åæ ‡
            'distance_km': float,        # è·ç¦»ï¼ˆå…¬é‡Œï¼‰
            'duration_hours': float      # æ—¶é•¿ï¼ˆå°æ—¶ï¼‰
        }
        æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    # æ£€æŸ¥ç¼“å­˜
    cache_key = f"{route_id}"
    if cache_key in ROUTE_CACHE:
        return ROUTE_CACHE[cache_key]
    
    try:
        # æ„å»ºOSRMè¯·æ±‚
        coords_str = f"{source_coords[0]},{source_coords[1]};{via_coords[0]},{via_coords[1]};{dest_coords[0]},{dest_coords[1]}"
        url = f"http://router.project-osrm.org/route/v1/driving/{coords_str}"
        params = {
            'overview': 'full',
            'geometries': 'geojson'
        }
        
        response = requests.get(url, params=params, timeout=10)
        
        if response.ok:
            data = response.json()
            if data.get('routes'):
                route = data['routes'][0]
                result = {
                    'path': route['geometry']['coordinates'],
                    'distance_km': route['distance'] / 1000,  # ç±³è½¬å…¬é‡Œ
                    'duration_hours': route['duration'] / 3600  # ç§’è½¬å°æ—¶
                }
                
                # ç¼“å­˜ç»“æœ
                ROUTE_CACHE[cache_key] = result
                return result
        
        print(f"  âš ï¸  OSRM failed for route {route_id}: {response.status_code}")
        return None
        
    except Exception as e:
        print(f"  âŒ Error getting OSRM route {route_id}: {e}")
        return None

def load_and_clean_data(csv_path):
    """Load and clean the full dataset"""
    print("Loading full dataset...")
    df = pd.read_csv(csv_path, encoding='utf-8-sig', low_memory=False)
    
    # Clean data
    df['year_clean'] = pd.to_numeric(df['year'], errors='coerce')
    df = df[df['year_clean'].between(2013, 2017)]
    
    for col in ['Source x', 'Source y', 'Destination x', 'Destination y']:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    df = df.dropna(subset=['Source x', 'Source y', 'Destination x', 'Destination y'])
    
    print(f"Loaded {len(df)} records")
    return df

def classify_flow_type(source_urban, dest_urban):
    """
    Classify flow type based on rural/urban status
    """
    source_is_urban = str(source_urban).lower() == 'yes'
    dest_is_urban = str(dest_urban).lower() == 'yes'
    
    if source_is_urban and dest_is_urban:
        return 'urban_to_urban'
    elif source_is_urban and not dest_is_urban:
        return 'urban_to_rural'
    elif not source_is_urban and dest_is_urban:
        return 'rural_to_urban'
    else:
        return 'rural_to_rural'

def get_flow_type_label(flow_type):
    """Get human-readable label"""
    labels = {
        'rural_to_urban': 'Rural â†’ Urban',
        'urban_to_rural': 'Urban â†’ Rural',
        'rural_to_rural': 'Rural â†’ Rural',
        'urban_to_urban': 'Urban â†’ Urban'
    }
    return labels.get(flow_type, 'Unknown')

def analyze_rural_urban_patterns(df):
    """Analyze rural-urban flow patterns"""
    print("\nAnalyzing rural-urban patterns...")
    
    # Add flow type classification
    df['flow_type'] = df.apply(
        lambda row: classify_flow_type(
            row['source_wit'], 
            row['destination_within_urban_boundary']
        ), 
        axis=1
    )
    
    # Overall statistics
    flow_type_counts = df['flow_type'].value_counts().to_dict()
    
    analysis = {
        'total_flows': len(df),
        'flow_patterns': {}
    }
    
    for flow_type, count in flow_type_counts.items():
        flow_df = df[df['flow_type'] == flow_type]
        
        analysis['flow_patterns'][flow_type] = {
            'label': get_flow_type_label(flow_type),
            'count': int(count),
            'percentage': round(count / len(df) * 100, 2),
            'total_quantity': float(flow_df['total_quantity'].sum()) if not flow_df['total_quantity'].isna().all() else 0,
            'avg_distance': float(flow_df['distance_1'].mean()) if 'distance_1' in flow_df else 0,
            'international_count': int(len(flow_df[flow_df['Crosses international border?'] == 'YES'])),
            'top_commodities': flow_df['commodit_1'].value_counts().head(5).to_dict()
        }
    
    return analysis, df


def build_city_coordinates_lookup(df):
    """
    Build a lookup table for city coordinates from source and destination data
    """
    print("Building city coordinates lookup table...")
    city_coords = {}
    
    # Get unique cities
    unique_cities = df['city'].dropna().unique()
    
    for city in unique_cities:
        # Try to find coordinates from source locations
        source_matches = df[df['source_nam'].str.contains(city, na=False, case=False)]
        if len(source_matches) > 0:
            # Use median coordinates for robustness
            coords = [
                float(source_matches['Source x'].median()),
                float(source_matches['Source y'].median())
            ]
            city_coords[city] = coords
            continue
        
        # Try to find coordinates from destination locations
        dest_matches = df[df['destination_name'].str.contains(city, na=False, case=False)]
        if len(dest_matches) > 0:
            coords = [
                float(dest_matches['Destination x'].median()),
                float(dest_matches['Destination y'].median())
            ]
            city_coords[city] = coords
    
    print(f"Found coordinates for {len(city_coords)} cities")
    return city_coords

def create_routes_with_rural_urban(df, min_flows=1):
    """
    Create aggregated routes with rural-urban classification
    Includes via_city information for three-point routes (source -> city -> destination)
    """
    print(f"Creating routes with rural-urban info (min {min_flows} flows)...")
    
    # Build city coordinates lookup
    city_coords_lookup = build_city_coordinates_lookup(df)
    
    routes = []
    
    # === æŒ‰ä¸‰ç‚¹è·¯çº¿èšåˆ: ä»¥åæ ‡ä¸ºå‡†ï¼ˆåå­—å¯é‡å¤ï¼‰===
    # åæ ‡èˆå…¥åˆ°å°æ•°ç‚¹å1ä½ï¼ˆçº¦11å…¬é‡Œç²¾åº¦ï¼‰ï¼Œèšåˆé™„è¿‘çš„ç‚¹
    
    # Add city to grouping (using fillna to handle missing values)
    df['city_grouped'] = df['city'].fillna('direct')
    
    # Round coordinates for aggregation (1 decimal place â‰ˆ 11km precision)
    # æ›´æ¿€è¿›çš„èšåˆï¼Œå‡å°‘æ›´å¤šè·¯çº¿
    df['src_x_rounded'] = df['Source x'].round(1)
    df['src_y_rounded'] = df['Source y'].round(1)
    df['dest_x_rounded'] = df['Destination x'].round(1)
    df['dest_y_rounded'] = df['Destination y'].round(1)
    
    grouped = df.groupby([
        'src_x_rounded',
        'src_y_rounded',
        'dest_x_rounded',
        'dest_y_rounded',
        'city_grouped',
        'flow_type'
    ])
    
    for key, route_df in grouped:
        src_x_rounded, src_y_rounded, dest_x_rounded, dest_y_rounded, city_name, flow_type = key
        
        if len(route_df) < min_flows:
            continue
        
        # ä½¿ç”¨èˆå…¥åçš„åæ ‡ï¼ˆä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
        src_x = src_x_rounded
        src_y = src_y_rounded
        dest_x = dest_x_rounded
        dest_y = dest_y_rounded
        
        # è·å–åå­—ï¼ˆå–ä¼—æ•°ï¼Œå› ä¸ºåŒä¸€åæ ‡å¯èƒ½æœ‰å¤šä¸ªåå­—ï¼‰
        source_name = route_df['source_nam'].mode()[0] if len(route_df['source_nam'].mode()) > 0 else 'Unknown'
        dest_name = route_df['destination_name'].mode()[0] if len(route_df['destination_name'].mode()) > 0 else 'Unknown'
        
        src_country = route_df['Source_country_name'].mode()[0] if len(route_df['Source_country_name'].mode()) > 0 else 'Unknown'
        dest_country = route_df['Dest_country_name'].mode()[0] if len(route_df['Dest_country_name'].mode()) > 0 else 'Unknown'
        
        total_flows = len(route_df)
        total_quantity = float(route_df['total_quantity'].sum()) if not route_df['total_quantity'].isna().all() else 0
        
        commodity_counts = route_df['commodit_1'].value_counts().head(5).to_dict()
        categories = route_df['commodit_2'].dropna().unique().tolist()
        years = sorted(route_df['year_clean'].dropna().unique().tolist())
        
        # Get transportation modes
        transport_modes = route_df['means_of_t'].value_counts().to_dict()
        main_transport = route_df['means_of_t'].mode()[0] if len(route_df['means_of_t'].mode()) > 0 else 'Unknown'
        
        # === NEW: Build detailed breakdowns by year and transport mode ===
        # By year breakdown
        by_year = {}
        for year in years:
            year_df = route_df[route_df['year_clean'] == year]
            year_commodities = year_df['commodit_1'].value_counts().to_dict()
            # Handle missing transport modes
            year_df_temp = year_df.copy()
            year_df_temp['means_of_t_clean'] = year_df_temp['means_of_t'].fillna('Unknown')
            year_transport = year_df_temp['means_of_t_clean'].value_counts().to_dict()
            by_year[int(year)] = {
                'flows': int(len(year_df)),
                'quantity': float(year_df['total_quantity'].sum()) if not year_df['total_quantity'].isna().all() else 0,
                'commodities': {str(k): int(v) for k, v in year_commodities.items()},
                'transport_modes': {str(k): int(v) for k, v in year_transport.items()}
            }
        
        # By transport mode breakdown (handle missing values)
        by_transport = {}
        # Create a temporary column with 'Unknown' for missing transport modes
        route_df_temp = route_df.copy()
        route_df_temp['means_of_t_clean'] = route_df_temp['means_of_t'].fillna('Unknown')
        
        # Get all transport modes including 'Unknown'
        all_transport_modes = route_df_temp['means_of_t_clean'].value_counts().to_dict()
        
        for transport in all_transport_modes.keys():
            trans_df = route_df_temp[route_df_temp['means_of_t_clean'] == transport]
            trans_years = sorted(trans_df['year_clean'].dropna().unique().tolist())
            trans_commodities = trans_df['commodit_1'].value_counts().to_dict()
            by_transport[str(transport)] = {
                'flows': int(len(trans_df)),
                'quantity': float(trans_df['total_quantity'].sum()) if not trans_df['total_quantity'].isna().all() else 0,
                'years': [int(y) for y in trans_years],
                'commodities': {str(k): int(v) for k, v in trans_commodities.items()}
            }
        
        # By commodity breakdown
        by_commodity = {}
        for commodity in commodity_counts.keys():
            comm_df = route_df[route_df['commodit_1'] == commodity]
            comm_years = sorted(comm_df['year_clean'].dropna().unique().tolist())
            # Handle missing transport modes
            comm_df_temp = comm_df.copy()
            comm_df_temp['means_of_t_clean'] = comm_df_temp['means_of_t'].fillna('Unknown')
            comm_transport = comm_df_temp['means_of_t_clean'].value_counts().to_dict()
            by_commodity[str(commodity)] = {
                'flows': int(len(comm_df)),
                'quantity': float(comm_df['total_quantity'].sum()) if not comm_df['total_quantity'].isna().all() else 0,
                'years': [int(y) for y in comm_years],
                'transport_modes': {str(k): int(v) for k, v in comm_transport.items()}
            }
        
        is_intl = (src_country != dest_country) if pd.notna(src_country) and pd.notna(dest_country) else False
        
        # Determine urban status
        source_is_urban = route_df['source_wit'].mode()[0] if len(route_df['source_wit'].mode()) > 0 else 'no'
        dest_is_urban = route_df['destination_within_urban_boundary'].mode()[0] if len(route_df['destination_within_urban_boundary'].mode()) > 0 else 'no'
        
        # Build route object (source_nameå’Œdest_nameå·²ç»ä»groupby keyä¸­è·å–)
        route = {
            'source': {
                'name': str(source_name),
                'country': str(src_country) if pd.notna(src_country) else 'Unknown',
                'coordinates': [float(src_x), float(src_y)],
                'is_urban': str(source_is_urban).lower() == 'yes'
            },
            'destination': {
                'name': str(dest_name),
                'country': str(dest_country) if pd.notna(dest_country) else 'Unknown',
                'coordinates': [float(dest_x), float(dest_y)],
                'is_urban': str(dest_is_urban).lower() == 'yes'
            },
            'flows': int(total_flows),
            'quantity': float(total_quantity),
            'commodities': {k: int(v) for k, v in commodity_counts.items()},
            'categories': [str(c) for c in categories],
            'years': [int(y) for y in years],
            'transport_modes': {str(k): int(v) for k, v in transport_modes.items()},
            'main_transport': str(main_transport),
            'is_international': bool(is_intl),
            'flow_type': flow_type,
            'flow_type_label': get_flow_type_label(flow_type),
            # === NEW: Detailed breakdowns for precise filtering ===
            'by_year': by_year,
            'by_transport': by_transport,
            'by_commodity': by_commodity
        }
        
        # Add via_city information if route goes through a city
        if city_name != 'direct' and city_name in city_coords_lookup:
            route['via_city'] = {
                'name': str(city_name),
                'coordinates': city_coords_lookup[city_name]
            }
        
        routes.append(route)
    
    print(f"Created {len(routes)} routes")
    return routes


def create_hierarchical_data_by_year(df):
    """
    Create hierarchical data structure organized by year
    
    Structure:
    {
      "2013": {
        "route_id_1": {
          "source": {...},
          "destination": {...},
          "via_city": {...},
          "flow": total_flows,
          "quantity": total_quantity,
          "commodity": {
            "Maize": {
              "category": "Cereal",
              "quantity": quantity,
              "transport": {
                "Truck": {
                  "quantity": quantity,
                  "flow": flow_count
                }
              }
            }
          },
          "is_international": true/false,
          "flow_type": "urban-urban",
          # "path": [...] # Will be added later with OSRM
        }
      }
    }
    """
    print("Creating hierarchical data structure by year...")
    
    # Build city coordinates lookup
    city_coords_lookup = build_city_coordinates_lookup(df)
    
    # Round coordinates for aggregation (1 decimal place â‰ˆ 11km precision)
    # æ›´æ¿€è¿›çš„èšåˆï¼Œå‡å°‘æ›´å¤šè·¯çº¿
    df['src_x_rounded'] = df['Source x'].round(1)
    df['src_y_rounded'] = df['Source y'].round(1)
    df['dest_x_rounded'] = df['Destination x'].round(1)
    df['dest_y_rounded'] = df['Destination y'].round(1)
    df['city_grouped'] = df['city'].fillna('direct')
    
    # Result structure: year -> route_id -> data
    data_by_year = defaultdict(dict)
    
    # Group by coordinates, city, flow_type, and year
    grouped = df.groupby([
        'src_x_rounded',
        'src_y_rounded',
        'dest_x_rounded',
        'dest_y_rounded',
        'city_grouped',
        'flow_type',
        'year_clean'
    ])
    
    for key, route_df in grouped:
        src_x, src_y, dest_x, dest_y, city_name, flow_type, year = key
        
        if pd.isna(year):
            continue
        
        year_str = str(int(year))
        
        # Create route_id based on coordinates
        route_id = f"r_{src_x}_{src_y}_{dest_x}_{dest_y}_{city_name}"
        
        # Get names (mode = most frequent)
        source_name = route_df['source_nam'].mode()[0] if len(route_df['source_nam'].mode()) > 0 else 'Unknown'
        dest_name = route_df['destination_name'].mode()[0] if len(route_df['destination_name'].mode()) > 0 else 'Unknown'
        src_country = route_df['Source_country_name'].mode()[0] if len(route_df['Source_country_name'].mode()) > 0 else 'Unknown'
        dest_country = route_df['Dest_country_name'].mode()[0] if len(route_df['Dest_country_name'].mode()) > 0 else 'Unknown'
        
        # Check if route_id already exists for this year (aggregate across all groups)
        if route_id not in data_by_year[year_str]:
            # Initialize route data
            data_by_year[year_str][route_id] = {
                'source': {
                    'name': source_name,
                    'coordinates': [float(src_x), float(src_y)],
                    'country': src_country,
                    'is_urban': str(route_df['source_wit'].mode()[0]).lower() == 'yes' if 'source_wit' in route_df.columns and len(route_df['source_wit'].mode()) > 0 else False
                },
                'destination': {
                    'name': dest_name,
                    'coordinates': [float(dest_x), float(dest_y)],
                    'country': dest_country,
                    'is_urban': str(route_df['destination_within_urban_boundary'].mode()[0]).lower() == 'yes' if 'destination_within_urban_boundary' in route_df.columns and len(route_df['destination_within_urban_boundary'].mode()) > 0 else False
                },
                'via_city': {
                    'name': city_name if city_name != 'direct' else None,
                    'coordinates': city_coords_lookup.get(city_name, [None, None]) if city_name != 'direct' else None
                },
                'flow': 0,
                'quantity': 0.0,
                'commodity': {},
                'is_international': src_country != dest_country,
                'flow_type': flow_type,
                # 'path': []  # Commented out for later OSRM processing
            }
        
        # Aggregate flows and quantity
        route_data = data_by_year[year_str][route_id]
        route_data['flow'] += len(route_df)
        route_data['quantity'] += float(route_df['total_quantity'].sum()) if not route_df['total_quantity'].isna().all() else 0
        
        # Process commodities
        for _, row in route_df.iterrows():
            commodity = row['commodit_1'] if pd.notna(row['commodit_1']) else 'Unknown'
            category = row['commodit_2'] if pd.notna(row['commodit_2']) else 'Unknown'
            transport = row['means_of_t'] if pd.notna(row['means_of_t']) else 'Unknown'
            quantity_val = float(row['total_quantity']) if pd.notna(row['total_quantity']) else 0
            
            # Initialize commodity if not exists
            if commodity not in route_data['commodity']:
                route_data['commodity'][commodity] = {
                    'category': category,
                    'quantity': 0.0,
                    'transport': {}
                }
            
            # Aggregate commodity quantity
            route_data['commodity'][commodity]['quantity'] += quantity_val
            
            # Initialize transport if not exists
            if transport not in route_data['commodity'][commodity]['transport']:
                route_data['commodity'][commodity]['transport'][transport] = {
                    'quantity': 0.0,
                    'flow': 0
                }
            
            # Aggregate transport data
            route_data['commodity'][commodity]['transport'][transport]['quantity'] += quantity_val
            route_data['commodity'][commodity]['transport'][transport]['flow'] += 1
    
    # Convert defaultdict to regular dict
    result = {year: dict(routes) for year, routes in data_by_year.items()}
    
    # Print summary
    total_routes = sum(len(routes) for routes in result.values())
    print(f"   Created {total_routes} routes across {len(result)} years")
    for year in sorted(result.keys()):
        print(f"     {year}: {len(result[year])} routes")
    
    return result


def main():
    """Main processing function"""
    import sys
    
    # Use the fixed CSV with corrected encoding
    csv_path = 'Karg_food_flows_locations_fixed.csv'
    use_osrm = '--osrm' in sys.argv
    skip_prompt = '--skip-prompt' in sys.argv or '--yes' in sys.argv or '-y' in sys.argv
    
    # Check if food_flows_by_year.json already exists
    hierarchical_file = 'food_flows_by_year_round1.json'  # round(1) = 11kmç²¾åº¦
    temp_file = 'food_flows_by_year_round1_temp.json'
    
    # Priority 1: Check for existing hierarchical data (skip data processing)
    if os.path.exists(hierarchical_file) and use_osrm:
        print("\n" + "="*70)
        print("ğŸ”„ FOUND EXISTING DATA FILE")
        print("="*70)
        print(f"\n   Found {hierarchical_file}")
        
        if skip_prompt:
            response = 'y'
            print(f"   Auto-loading existing data (--skip-prompt)")
        else:
            response = input(f"   Load existing data and only update OSRM paths? (y/n): ")
        
        if response.lower() == 'y':
            print(f"   Loading existing hierarchical data...")
            with open(hierarchical_file, 'r', encoding='utf-8') as f:
                hierarchical_data = json.load(f)
            
            total_routes = sum(len(routes) for routes in hierarchical_data.values())
            print(f"   âœ“ Loaded {total_routes} routes from {len(hierarchical_data)} years")
            
            # Convert hierarchical data back to flat all_routes (for OSRM processing)
            print(f"   Converting to flat route list...")
            all_routes = []
            for year_str, year_routes in hierarchical_data.items():
                for route_id, route_data in year_routes.items():
                    # Check if this route already exists in all_routes
                    existing = next((r for r in all_routes 
                                   if r['source']['coordinates'] == route_data['source']['coordinates'] 
                                   and r['destination']['coordinates'] == route_data['destination']['coordinates']
                                   and r.get('via_city', {}).get('name') == route_data.get('via_city', {}).get('name')), 
                                   None)
                    
                    if existing:
                        # Merge years
                        if int(year_str) not in existing['years']:
                            existing['years'].append(int(year_str))
                        # Copy path if exists
                        if 'path' in route_data and route_data['path']:
                            existing['path'] = route_data['path']
                            existing['distance_km'] = route_data.get('distance_km')
                            existing['duration_hours'] = route_data.get('duration_hours')
                    else:
                        # Create new route entry
                        new_route = {
                            'source': route_data['source'],
                            'destination': route_data['destination'],
                            'via_city': route_data.get('via_city'),
                            'flows': route_data['flow'],
                            'quantity': route_data['quantity'],
                            'is_international': route_data['is_international'],
                            'flow_type': route_data['flow_type'],
                            'years': [int(year_str)]
                        }
                        if 'path' in route_data and route_data['path']:
                            new_route['path'] = route_data['path']
                            new_route['distance_km'] = route_data.get('distance_km')
                            new_route['duration_hours'] = route_data.get('duration_hours')
                        all_routes.append(new_route)
            
            print(f"   âœ“ Converted to {len(all_routes)} unique routes")
            
            # Create dummy analysis for summary
            overall_analysis = {'flow_patterns': {}}
            df_with_types = None
            
            print(f"   âœ“ Ready to update OSRM paths (skipped CSV processing)\n")
            # Continue to OSRM section
        else:
            print(f"   Starting fresh analysis...")
            df = load_and_clean_data(csv_path)
            overall_analysis, df_with_types = analyze_rural_urban_patterns(df)
            all_routes = create_routes_with_rural_urban(df_with_types, min_flows=1)
            hierarchical_data = create_hierarchical_data_by_year(df_with_types)
    
    # Priority 2: Check for temporary file (resume from interruption)
    elif use_osrm and os.path.exists(temp_file):
        print("\n" + "="*70)
        print("ğŸ”„ RESUMING FROM PREVIOUS RUN")
        print("="*70)
        print(f"\n   Found temporary file: {temp_file}")
        
        if skip_prompt:
            response = 'y'
            print(f"   Auto-resuming from temporary file (--skip-prompt)")
        else:
            response = input(f"   Load partial results and continue? (y/n): ")
        
        if response.lower() == 'y':
            print(f"   Loading partial hierarchical data...")
            with open(temp_file, 'r', encoding='utf-8') as f:
                hierarchical_data = json.load(f)
            
            # Load original data and rebuild all_routes for OSRM processing
            df = load_and_clean_data(csv_path)
            overall_analysis, df_with_types = analyze_rural_urban_patterns(df)
            all_routes = create_routes_with_rural_urban(df_with_types, min_flows=1)
            
            total_routes = sum(len(routes) for routes in hierarchical_data.values())
            print(f"   âœ“ Loaded hierarchical data ({total_routes} routes)")
            print(f"   Continuing OSRM processing...\n")
            
            # Skip to OSRM processing section
            import sys
            sys.argv.append('--resume')
            # Will be handled in OSRM section below
        else:
            print(f"   Starting fresh analysis...")
            df = load_and_clean_data(csv_path)
            print("\n" + "="*70)
            print("RURAL-URBAN FOOD FLOWS ANALYSIS")
            print("="*70 + "\n")
            overall_analysis, df_with_types = analyze_rural_urban_patterns(df)
            print("   Flow Pattern Distribution:")
            for flow_type, data in overall_analysis['flow_patterns'].items():
                print(f"   {data['label']:20s}: {data['count']:6,} ({data['percentage']:5.2f}%)")
            print("\n2. Creating Route Files")
            all_routes = create_routes_with_rural_urban(df_with_types, min_flows=1)
            hierarchical_data = create_hierarchical_data_by_year(df_with_types)
    else:
        # Normal flow - fresh start
        df = load_and_clean_data(csv_path)
        
        print("\n" + "="*70)
        print("RURAL-URBAN FOOD FLOWS ANALYSIS")
        print("="*70 + "\n")
        
        # Overall rural-urban pattern analysis (needed for flow_type classification)
        print("1. Analyzing rural-urban patterns...")
        overall_analysis, df_with_types = analyze_rural_urban_patterns(df)
        
        # Print summary
        print("   Flow Pattern Distribution:")
        for flow_type, data in overall_analysis['flow_patterns'].items():
            print(f"   {data['label']:20s}: {data['count']:6,} ({data['percentage']:5.2f}%)")
        
        # Create route files
        print("\n2. Creating Route Files")
        
        # All routes (only file needed for visualization)
        all_routes = create_routes_with_rural_urban(df_with_types, min_flows=1)
        
        # NEW: Create hierarchical data structure by year
        print("\n3. Creating Hierarchical Data Structure by Year")
        hierarchical_data = create_hierarchical_data_by_year(df_with_types)
    
    # OSRMè·¯å¾„è·å– (å¯é€‰)
    import sys
    use_osrm = '--osrm' in sys.argv
    top_100_only = '--top100' in sys.argv
    
    if use_osrm:
        print("\n3. Fetching OSRM Real Route Paths...")
        load_route_cache()
        
        # åŠ è½½ä¹‹å‰çš„è¿›åº¦
        previous_progress = load_progress()
        start_from = 0
        if previous_progress:
            response = input(f"   Continue from previous progress ({previous_progress['processed']}/{previous_progress['total']})? (y/n): ")
            if response.lower() == 'y':
                start_from = previous_progress['processed']
        
        # é€‰æ‹©è¦å¤„ç†çš„routes
        if top_100_only:
            routes_to_process = sorted(all_routes, key=lambda x: x['flows'], reverse=True)[:100]
            print(f"   Processing TOP 100 routes only")
        else:
            routes_to_process = all_routes
            print(f"   Processing ALL {len(all_routes)} routes")
        
        success_count = 0
        fail_count = 0
        batch_size = 50  # æ¯50ä¸ªä¿å­˜ä¸€æ¬¡ç¼“å­˜å’Œä¸­é—´ç»“æœ
        idx = start_from  # Initialize idx for error handling
        start_time = time.time()
        
        print(f"\n   Starting from route {start_from + 1}/{len(routes_to_process)}")
        print(f"   Cache will be saved every {batch_size} routes")
        print(f"   Press Ctrl+C to safely stop (progress will be saved)\n")
        
        try:
            for idx, route in enumerate(routes_to_process, 1):
                # è·³è¿‡å·²å¤„ç†çš„
                if idx <= start_from:
                    # æ£€æŸ¥æ˜¯å¦å·²æœ‰pathæ•°æ®
                    if 'path' in route:
                        success_count += 1
                    continue
                
                # âœ… å…³é”®ä¿®å¤ï¼šè·³è¿‡å·²æœ‰è·¯å¾„çš„è·¯çº¿
                if 'path' in route and route['path']:
                    success_count += 1
                    print_progress_bar(idx, len(routes_to_process), success_count, fail_count, start_time, 'OSRM')
                    continue
                
                # åªå¤„ç†æœ‰via_cityçš„è·¯çº¿
                if 'via_city' not in route or not route['via_city'] or not route['via_city'].get('coordinates'):
                    # æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆè·³è¿‡æ­¤è·¯çº¿ï¼‰
                    print_progress_bar(idx, len(routes_to_process), success_count, fail_count, start_time, 'OSRM')
                    continue
                
                source_coords = route['source']['coordinates']
                via_coords = route['via_city']['coordinates']
                dest_coords = route['destination']['coordinates']
                route_id = f"{route['source']['name']}-{route['via_city']['name']}-{route['destination']['name']}"
                
                # è·å–OSRMè·¯å¾„
                osrm_result = get_osrm_route(source_coords, via_coords, dest_coords, route_id)
                
                if osrm_result:
                    # ç®€åŒ–è·¯å¾„ï¼šä»å¹³å‡ 2919 ç‚¹å‡å°‘åˆ° ~50 ç‚¹
                    original_points = len(osrm_result['path'])
                    simplified_path = simplify_path(osrm_result['path'], epsilon=0.001)
                    
                    route['path'] = simplified_path
                    route['distance_km'] = osrm_result['distance_km']
                    route['duration_hours'] = osrm_result['duration_hours']
                    route['path_points_original'] = original_points  # è®°å½•åŸå§‹ç‚¹æ•°
                    route['path_points_simplified'] = len(simplified_path)
                    success_count += 1
                else:
                    # å¤±è´¥æ—¶ä¿ç•™ç›´çº¿ï¼ˆä¸æ·»åŠ pathå­—æ®µï¼Œå‰ç«¯ä¼šfallbackåˆ°ArcLayerï¼‰
                    fail_count += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                print_progress_bar(idx, len(routes_to_process), success_count, fail_count, start_time, 'OSRM')
                
                # åˆ†æ‰¹ä¿å­˜ç¼“å­˜ã€è¿›åº¦å’Œä¸­é—´ç»“æœ
                if idx % batch_size == 0:
                    print()  # æ–°è¡Œ
                    save_route_cache()
                    save_progress(idx, len(routes_to_process), route_id)
                    save_intermediate_results(hierarchical_data)
                    print(f"   ğŸ’¾ Checkpoint saved at {idx}/{len(routes_to_process)}")
                
                # é™æµï¼šæ¯æ¬¡è¯·æ±‚é—´éš”0.1ç§’
                time.sleep(0.1)
        
        except KeyboardInterrupt:
            print(f"\n\nâš ï¸  Interrupted by user!")
            elapsed = time.time() - start_time
            elapsed_str = str(timedelta(seconds=int(elapsed)))
            print(f"   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            print(f"   Processed: {idx}/{len(routes_to_process)} ({idx/len(routes_to_process)*100:.1f}%)")
            print(f"   Success:   {success_count} | Failed: {fail_count}")
            print(f"   Time:      {elapsed_str}")
            print(f"   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            print(f"\n   Saving progress before exit...")
            save_route_cache()
            save_progress(idx, len(routes_to_process), route_id if 'route_id' in locals() else 'interrupted')
            save_intermediate_results(hierarchical_data)
            print(f"   âœ“ Progress saved. Run script again to continue from route {idx}")
            print(f"   âœ“ Partial results saved to food_flows_by_year_temp.json")
            sys.exit(0)
        
        except Exception as e:
            print(f"\n\nâŒ Error occurred: {e}")
            print(f"   Saving progress before exit...")
            save_route_cache()
            save_progress(idx, len(routes_to_process), route_id if 'route_id' in locals() else 'error')
            save_intermediate_results(hierarchical_data)
            print(f"   âœ“ Progress saved to resume later")
            raise
        
        # å®Œæˆåæ˜¾ç¤ºæœ€ç»ˆè¿›åº¦æ¡
        print_progress_bar(len(routes_to_process), len(routes_to_process), success_count, fail_count, start_time, 'OSRM')
        print()  # æ–°è¡Œ
        
        # æœ€åä¿å­˜ä¸€æ¬¡
        save_route_cache()
        save_progress(len(routes_to_process), len(routes_to_process), 'completed')
        
        # è®¡ç®—æ€»è€—æ—¶
        total_time = time.time() - start_time
        total_time_str = str(timedelta(seconds=int(total_time)))
        
        print(f"\n   âœ… OSRM Processing Complete!")
        print(f"   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"   âœ“ Success:      {success_count:,} routes")
        print(f"   âœ— Failed:       {fail_count:,} routes")
        if success_count + fail_count > 0:
            print(f"   ğŸ“Š Success Rate: {success_count/(success_count+fail_count)*100:.1f}%")
        print(f"   â±ï¸  Total Time:   {total_time_str}")
        print(f"   ğŸš€ Average Speed: {len(routes_to_process)/total_time:.2f} routes/s")
        print(f"   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        
        # å®Œæˆåæ¸…ç†è¿›åº¦æ–‡ä»¶
        if os.path.exists(PROGRESS_FILE):
            os.remove(PROGRESS_FILE)
            print(f"   âœ“ Progress tracking completed, removed {PROGRESS_FILE}")
        
        # é‡è¦ï¼šå°† OSRM è·¯å¾„æ•°æ®åŒæ­¥åˆ° hierarchical_data
        print(f"\n   Updating hierarchical data with OSRM paths...")
        paths_added = 0
        total_points_before = 0
        total_points_after = 0
        
        for route in all_routes:
            if 'path' in route and route['path']:
                # Find this route in hierarchical_data and add path
                # ä½¿ç”¨ round(1) ä¸èšåˆç²¾åº¦ä¿æŒä¸€è‡´
                src_x = round(route['source']['coordinates'][0], 1)
                src_y = round(route['source']['coordinates'][1], 1)
                dest_x = round(route['destination']['coordinates'][0], 1)
                dest_y = round(route['destination']['coordinates'][1], 1)
                
                # Determine city_name
                if route.get('via_city') and route['via_city'].get('name'):
                    city_name = route['via_city']['name']
                else:
                    city_name = 'direct'
                
                route_id = f"r_{src_x}_{src_y}_{dest_x}_{dest_y}_{city_name}"
                
                # Add path to all years this route appears in
                for year in route.get('years', []):
                    year_str = str(year)
                    if year_str in hierarchical_data and route_id in hierarchical_data[year_str]:
                        hierarchical_data[year_str][route_id]['path'] = route['path']
                        hierarchical_data[year_str][route_id]['distance_km'] = route.get('distance_km')
                        hierarchical_data[year_str][route_id]['duration_hours'] = route.get('duration_hours')
                        paths_added += 1
                        
                        # ç»Ÿè®¡ç®€åŒ–æ•ˆæœ
                        if 'path_points_original' in route:
                            total_points_before += route['path_points_original']
                            total_points_after += route['path_points_simplified']
        
        print(f"   âœ“ Added {paths_added} OSRM paths to hierarchical data")
        if total_points_before > 0:
            reduction = (1 - total_points_after / total_points_before) * 100
            print(f"   âœ“ Path simplification: {total_points_before:,} â†’ {total_points_after:,} points ({reduction:.1f}% reduction)")
    
    # ä¿å­˜JSONæ–‡ä»¶
    print("\n4. Saving Final Output File")
    
    try:
        # Only save hierarchical format by year (the one we actually use)
        # hierarchical_file already defined at the top of main()
        print(f"   Saving {hierarchical_file}...")
        with open(hierarchical_file, 'w', encoding='utf-8') as f:
            json.dump(hierarchical_data, f, indent=2, ensure_ascii=False)
        
        total_routes_hierarchical = sum(len(routes) for routes in hierarchical_data.values())
        print(f"   âœ“ Saved {hierarchical_file} ({total_routes_hierarchical} routes across {len(hierarchical_data)} years)")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        # temp_file already defined at the top of main()
        if os.path.exists(temp_file):
            os.remove(temp_file)
            print(f"   âœ“ Cleaned up temporary file")
        
    except Exception as e:
        print(f"\nâŒ Error saving files: {e}")
        print(f"   Check intermediate results in *_temp.json files")
        raise
    
    print("\n" + "="*70)
    print("âœ… RURAL-URBAN ANALYSIS COMPLETE!")
    print("="*70)
    
    if df_with_types is not None:
        print(f"\nKey Findings:")
        print(f"  Total flows analyzed: {len(df_with_types):,}")
        if overall_analysis.get('flow_patterns'):
            for flow_type, data in overall_analysis['flow_patterns'].items():
                print(f"  {data['label']:20s}: {data['count']:6,} flows ({data['percentage']:5.2f}%)")
    
    print(f"\nOutput File:")
    print(f"  - {hierarchical_file}")
    
    if use_osrm:
        # Count routes with paths
        routes_with_paths = sum(1 for year_routes in hierarchical_data.values() 
                               for route in year_routes.values() 
                               if 'path' in route and route['path'])
        total_routes = sum(len(routes) for routes in hierarchical_data.values())
        print(f"  - Total routes: {total_routes:,}")
        print(f"  - Routes with OSRM paths: {routes_with_paths:,} ({routes_with_paths/total_routes*100:.1f}%)")
    
    if use_osrm:
        print(f"\nCache Files:")
        print(f"  - {CACHE_FILE} (OSRM route cache, {len(ROUTE_CACHE):,} cached routes)")
        print(f"  - Can be safely deleted after completion")

if __name__ == '__main__':
    main()

